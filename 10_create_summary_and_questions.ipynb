{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, fnmatch\n",
    "\n",
    "\n",
    "import src.valid_index\n",
    "importlib.reload(src.valid_index)\n",
    "from src.valid_index import get_banking_act_index\n",
    "\n",
    "import src.file_tools\n",
    "importlib.reload(src.file_tools)\n",
    "from src.file_tools import read_processed_regs_into_dataframe, get_regulation_detail\n",
    "\n",
    "import src.embeddings\n",
    "importlib.reload(src.embeddings)\n",
    "from src.embeddings import get_ada_embedding, num_tokens_from_string\n",
    "\n",
    "import src.tree_tools\n",
    "importlib.reload(src.tree_tools)\n",
    "from src.tree_tools import build_tree_for_regulation, split_tree\n",
    "\n",
    "\n",
    "index_reg23 = get_banking_act_index()\n",
    "\n",
    "non_text_labels = ['Table', 'Formula', 'Example', 'Definition']\n",
    "dir_path = './txt/'\n",
    "file_list = []\n",
    "for root, dir, files in os.walk(dir_path):\n",
    "    for file in files:\n",
    "        str = 'reg23_sections*.txt'\n",
    "        if fnmatch.fnmatch(file, str):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append(file_path)\n",
    "\n",
    "df_reg23, non_text = read_processed_regs_into_dataframe(file_list=file_list, valid_index_checker=index_reg23, non_text_labels=non_text_labels)\n",
    "tree_reg23 = build_tree_for_regulation(\"BA\", df_reg23, valid_index_checker=index_reg23)\n",
    "\n",
    "\n",
    "section_summary_with_embeddings = \"./tmp/summary_reg23_with_embedding.parquet\"\n",
    "section_questions_with_embeddings = \"./tmp/summary_reg23_questions_with_embedding.parquet\"\n",
    "headings_index_file = \"./tmp/headings_reg23.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the initial split of the tree. You will need to make changes to this as you see the data\n",
      "Total number of sections: 461\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sectioned_df = pd.DataFrame([],columns = [\"section\", \"text\", \"token_count\"])\n",
    "save_sectioned_df_to_file = \"./tmp/banking_act.csv\"\n",
    "exists = os.path.exists(save_sectioned_df_to_file)\n",
    "\n",
    "if not exists:\n",
    "    print(\"Loading the initial split of the tree. You will need to make changes to this as you see the data\")\n",
    "    # Starting at an particular parent node (can be the tree root or any child), this method splits up the \n",
    "    # branch into sections where the text does not exceed a certain word_count cap.\n",
    "    sectioned_df = split_tree(tree_reg23.root, df_reg23, 700, index_reg23)\n",
    "else:\n",
    "    print(\"Loading the currency split of the tree so you can continue generating summaries and questions\")\n",
    "    sectioned_df = pd.read_csv(save_sectioned_df_to_file, encoding=\"utf-8\", sep=\"|\")\n",
    "\n",
    "print(f'Total number of sections: {len(sectioned_df)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create or load the DataFrames that will hold the text index. Later we will add the embeddings to the same DataFrames. When we do so, loading the embeddings is slow from certain file formats like csv so we just start with a fast loading file format - parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data contains 9 lines of text\n",
      "Questions data contains 9 lines of text\n",
      "There are a total number of 461 sections to index\n",
      "You have created 1.95 percent of your text index\n"
     ]
    }
   ],
   "source": [
    "df_summary = None\n",
    "if os.path.exists(section_summary_with_embeddings):\n",
    "    df_summary = pd.read_parquet(section_summary_with_embeddings, engine='pyarrow')\n",
    "    print(f\"Summary data contains {len(df_summary)} lines of text\")\n",
    "    missing = len(df_summary[df_summary[\"text\"] == \"\"])\n",
    "    if missing > 0:\n",
    "        print(f\" -- of which there are {missing} lines that do not contain index text (e.g. sections with only definitions or indexes)\")\n",
    "else:\n",
    "    print(\"Creating a new summary DataFrame\")\n",
    "    df_summary = pd.DataFrame([], columns = [\"text\", \"section\"])\n",
    "\n",
    "\n",
    "df_questions = None\n",
    "if os.path.exists(section_questions_with_embeddings):\n",
    "    df_questions = pd.read_parquet(section_questions_with_embeddings, engine='pyarrow')\n",
    "    print(f\"Questions data contains {len(df_questions)} lines of text\")    \n",
    "    missing = len(df_questions[df_questions[\"text\"] == \"\"])\n",
    "    if missing > 0:\n",
    "        print(f\" -- of which there are {missing} lines that do not contain index text (e.g. sections with only definitions or indexes)\")\n",
    "else:\n",
    "    print(\"Creating a new questions DataFrame\")\n",
    "    df_questions = pd.DataFrame([], columns = [\"text\", \"section\"])\n",
    "\n",
    "index = None\n",
    "if len(df_summary) != len(df_questions):\n",
    "    print(\"The summary and the questions DataFrames do not have the same length\")\n",
    "else:\n",
    "    index = len(df_summary)\n",
    "    p = (index / len(sectioned_df)) * 100\n",
    "    print(f'There are a total number of {len(sectioned_df)} sections to index')\n",
    "    print(f\"You have created {p:.2f} percent of your text index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) Calculation of credit risk exposure: standardised approach\n",
      "Subject to the relevant provisions of regulation 38(2) and subregulation (20), a bank that adopted the standardised approach for the measurement of the bank's exposure to credit risk\n",
      "    (b) shall in a consistent manner, in accordance with the relevant requirements specified below, and in terms of the bank 's internal risk management process, apply the ratings or assessments issued by an eligible external credit assessment institution of the bank's choice, or export credit agency, to calculate the bank's risk exposure in terms of the relevant provisions contained in these Regulations, that is, the bank shall not \"cherry pick\" ratings or assessments issued by different external credit assessment institutions, arbitrarily change the use of eligible external credit assessment institutions of apply ratings of assessments for purposes of these Regulations differently from the bank's Internal risk management process.\n",
      "        (i) Multiple assessments\n",
      "        When a bank has a choice between-\n",
      "            (A) two assessments issued by eligible external credit assessment institutions, which assessments relate to different risk weighting categories, the higher of the two week weights shall apply,\n",
      "            (B) three or more assessments issued by eligible external credit assessment institutions, which assessments relate to different risk weighting categories, the higher of the lowest two risk weights shall apply.\n"
     ]
    }
   ],
   "source": [
    "#index = 120\n",
    "print(get_regulation_detail(\"23(5)(b)(i)\", df_reg23, index_reg23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8QAEtDiyP3H76dRmMRN5bBm2kbI6w', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"In the simplified standardised approach, if I obtain eligible collateral, guarantees, or a netting agreement that effectively transfers risk, I can reduce my credit risk exposure. Any transaction with credit protection cannot be assigned a higher risk weight than a similar transaction without protection.\\n\\nFor securitisation exposure, I can only recognize protection if it meets certain conditions. Eligible collateral must qualify as per specific provisions, and guarantees and credit-derivative instruments can only come from approved providers that meet minimum requirements. However, special-purpose institutions connected to the securitisation cannot be considered eligible protection providers. For the portions of exposure that are protected and unprotected, I need to maintain capital requirements following the set requirements.\\n\\nWhen dealing with maturity mismatches in securitisation, I must maintain a capital requirement for the protected part based on the specific provisions, considering the longest maturity if there are varying maturities. If I'm providing protection as an outsider and not the originator for a securitisation exposure, especially if it relates to an unrated credit-enhancement facility, I should treat the exposure as if I'm directly providing an unrated credit-enhancement facility for the securitisation scheme.\", role='assistant', function_call=None, tool_calls=None))], created=1701246579, model='gpt-4-1106-preview', object='chat.completion', system_fingerprint='fp_a24b4d720c', usage=CompletionUsage(completion_tokens=240, prompt_tokens=891, total_tokens=1131))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "system_content_summerise = \"You are summarising parts of Regulation 23 of the Banks Act (Reg23) for a bank that needs to complete the Credit Risk Monthly Return (Form BA 200). When summerising, do not add filler words like 'the act says ...' or 'Reg23 says ...', just summarise the section. Your summary should use plain language and avoid legalese. Since it is for a bank, when the act uses the phrase 'bank', please replace it with the relevant first person pronoun like 'I' or 'me'. A good summary will minimise the use of lists. Rather it should make use of paragraphs.\\n\\\n",
    "Note: Reg23 offers two approaches to modelling credit risk namely the standardised approach and the internal ratings-based (IRB) approach. The standardised approach is subdivided into the 'Simplified Standardised' and the 'Standardised' approach. The IRB approach is subdivided into the 'Foundation IRB' (FIRB) and the 'Advanced IRB' (AIRB). The act will often refer to Method 1 or Method 2 but please use the full name or acronym of the appropriate calculation methodology in your summary.\"\n",
    "\n",
    "user_context = get_regulation_detail(\"23(7)(e)\", df_reg23, index_reg23)\n",
    "response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    temperature = 1.0,\n",
    "                    max_tokens = 500,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_content_summerise},\n",
    "                        {\"role\": \"user\", \"content\": user_context},\n",
    "                    ]\n",
    "                )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the text index \n",
    "\n",
    "This is a manual process. We call OpenAI and print out the answers in a format that is used to update the index text but this does need to be edited before it is added to the index so this is not automated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "(5) Calculation of credit risk exposure: standardised approach\n",
      "Subject to the relevant provisions of regulation 38(2) and subregulation (20), a bank that adopted the standardised approach for the measurement of the bank's exposure to credit risk\n",
      "    (d) shall comply with the relevant requirements specified in subregulations (6) to (9) below\n",
      "##############\n",
      "df_summary.loc[index, \"section\"] = \"23(5)(d)\"\n",
      "df_summary.loc[index, \"text\"] = \"Using the Standardised approach to measure my credit risk exposure, I must comply with certain requirements specified in sections (6) to (9) of Regulation 23, as long as they align with the provisions in regulation 38(2) and section (20).\"\n",
      "\n",
      "df_questions.loc[index, \"section\"] = \"23(5)(d)\"\n",
      "df_questions.loc[index, \"text\"] = \"What approach should I follow to measure credit risk exposure according to Regulation 23? | In complying with Regulation 23, which sections outline the requirements for measuring credit risk using the Standardised approach?\"\n"
     ]
    }
   ],
   "source": [
    "import src.summarise_and_question\n",
    "importlib.reload(src.summarise_and_question)\n",
    "from src.summarise_and_question import get_summary_and_questions_for\n",
    "\n",
    "\n",
    "#model = \"gpt-3.5-turbo\"\n",
    "#model=\"gpt-4\"\n",
    "model=\"gpt-4-1106-preview\"\n",
    "\n",
    "reg_text = sectioned_df.loc[index]['text']\n",
    "print(\"##############\")\n",
    "print(reg_text)\n",
    "print(\"##############\")\n",
    "\n",
    "model_summary, model_questions = get_summary_and_questions_for(reg_text, model = model)\n",
    "\n",
    "#format output\n",
    "section = sectioned_df.loc[index]['section']\n",
    "print(f'df_summary.loc[index, \"section\"] = \"{section}\"')\n",
    "print(f'df_summary.loc[index, \"text\"] = \"{model_summary}\"')\n",
    "print()\n",
    "print(f'df_questions.loc[index, \"section\"] = \"{section}\"')\n",
    "print(f'df_questions.loc[index, \"text\"] = \"{model_questions}\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next section is 23(6)(a) which is on line 12\n",
      "You have completed 2.60 percent of your work\n",
      "Next section\n",
      "##############\n",
      "(6) Method 1: Calculation of credit risk exposure in terms of the simplified standardised approach\n",
      "Unless specifically otherwise provided in these Regulations, a bank that adopted the simplified standardised approach for the measurement of the bank's exposure to credit risk arising from positions held in its banking book shall risk weight its relevant exposure, net of any credit impairment, in accordance with the relevant requirements specified below:\n",
      "    (a) In the case of exposure to sovereigns, central banks, public-sector entities, banks, securities firms and corporate institutions, in accordance with the provisions of table 1 below.\n",
      "##############\n"
     ]
    }
   ],
   "source": [
    "# Once the last four lines are manually checked, the edited result is copied here and the summary and question index is updated\n",
    "\n",
    "df_summary.loc[index, \"section\"] = \"23(5)(d)\"\n",
    "df_summary.loc[index, \"text\"] = \"Using the Standardised approach to measure my credit risk exposure, I must comply with certain requirements specified in sections (6) to (9) of Regulation 23.\"\n",
    "\n",
    "df_questions.loc[index, \"section\"] = \"23(5)(d)\"\n",
    "df_questions.loc[index, \"text\"] = \"Which sections outline the requirements for measuring credit risk using the Standardised approach?\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "index = index + 1\n",
    "if index == len(sectioned_df):\n",
    "    print(\"All done!\")\n",
    "else:\n",
    "    next_section = sectioned_df.iloc[index][\"section\"]\n",
    "    assert len(sectioned_df[sectioned_df[\"section\"] == next_section]) == 1, \"Huston, we have a problem\"\n",
    "    print(f'Next section is {next_section} which is on line {index}')\n",
    "    p = ((index) / len(sectioned_df)) * 100\n",
    "    print(f\"You have completed {p:.2f} percent of your work\")\n",
    "    reg_text = sectioned_df.loc[index]['text']\n",
    "    print(\"Next section\")\n",
    "    print(\"##############\")\n",
    "    print(reg_text)\n",
    "    print(\"##############\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes there are errors in the previous code block. We need to be careful when saving over any work we have already done so the \n",
    "# save step is a manual one which needs to be run regularly but without overwriting good data with bad data\n",
    "df_summary.to_parquet(section_summary_with_embeddings, engine='pyarrow')\n",
    "df_questions.to_parquet(section_questions_with_embeddings, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>The Minister has the authority to establish fe...</td>\n",
       "      <td>111.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>The Minister has the authority to make regulat...</td>\n",
       "      <td>112.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Before creating or changing regulations, the M...</td>\n",
       "      <td>113.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>You have one year from the commencement of thi...</td>\n",
       "      <td>114.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>The name of the Act is the Protection of Perso...</td>\n",
       "      <td>115.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text section\n",
       "135  The Minister has the authority to establish fe...    111.\n",
       "136  The Minister has the authority to make regulat...    112.\n",
       "137  Before creating or changing regulations, the M...    113.\n",
       "138  You have one year from the commencement of thi...    114.\n",
       "139  The name of the Act is the Protection of Perso...    115."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_summary.drop(49, inplace=True)\n",
    "df_summary[134:145]\n",
    "\n",
    "#df_questions[40:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Some of the values in df_summary are pipe separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing how the sections are chunked\n",
    "\n",
    "From time to time we will see instances whe the initial chunk size needs to be adjusted and nodes need to be expanded or collapsed. We do this in two stages. First we can experiment with a new \"token_limit_per_chunk\" for the node in question and once we find the chunking solution we are after, we remove the old chunks and replace them with the new chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will remove 9 row(s) from sectioned_df. From 82 to 91\n",
      "... and will replace them with 1 row(s)\n",
      "     section                                               text  token_count\n",
      "0  C.1(D)(v)  C.1 FinSurv Reporting System\\n    (D) Offshori...         1208\n"
     ]
    }
   ],
   "source": [
    "node_str = \"C.1(D)(v)\"\n",
    "# get the list of indicies that start with this string\n",
    "index_list = sectioned_df.index[sectioned_df['section'].str.startswith(node_str)].tolist()\n",
    "is_consecutive = all(x+1 == y for x, y in zip(index_list[:-1], index_list[1:]))\n",
    "assert is_consecutive, \"The list of indicies that start with the node string is not consecutive so the rest of the logic here will not hold\"\n",
    "start_index_to_replace = index_list[0]\n",
    "end_index_to_replace = index_list[-1] + 1\n",
    "print(f\"This will remove {len(index_list)} row(s) from sectioned_df. From {start_index_to_replace} to {end_index_to_replace}\")\n",
    "\n",
    "# Get the new set of indicies assuming a different chunking length\n",
    "node = tree_reg23.get_node(node_str)\n",
    "token_limit_per_chunk = 1300\n",
    "\n",
    "tmp_df = split_tree(node, df_reg23, token_limit_per_chunk, index_reg23)\n",
    "\n",
    "print(f\"... and will replace them with {len(tmp_df)} row(s)\")\n",
    "print(tmp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the node and all its children with the new DataFrame with a different word_count limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rows(original_df, updated_section_df, start_row, end_row):\n",
    "    before = original_df.iloc[:start_row]\n",
    "    after = original_df.iloc[end_row:]\n",
    "    new_df = pd.concat([before, updated_section_df, after]).reset_index(drop=True)\n",
    "    return new_df\n",
    "\n",
    "print(f\"The original data consisted of {sectioned_df} chunks\")\n",
    "sectioned_df = replace_rows(sectioned_df, tmp_df, start_row=start_index_to_replace, end_row=end_index_to_replace)\n",
    "print(f\"Post the update, the data consists of {sectioned_df} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jesus saves! \n",
    "But only if he is happy with the results. Check first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.</td>\n",
       "      <td>2. Purpose of Act\\nThe purpose of this Act is ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.</td>\n",
       "      <td>3. Application and interpretation of Act\\n    ...</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.</td>\n",
       "      <td>4. Lawful processing of personal information\\n...</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.</td>\n",
       "      <td>5. Rights of data subjects\\nA data subject has...</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.</td>\n",
       "      <td>6. Exclusions\\n    (1) This Act does not apply...</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>111.</td>\n",
       "      <td>111. Fees\\n    (1) The Minister may, subject t...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>112.</td>\n",
       "      <td>112. Regulations\\n    (1) The Minister may, su...</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>113.</td>\n",
       "      <td>113. Procedure for making regulations\\n    (1)...</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>114.</td>\n",
       "      <td>114. Transitional arrangements\\n    (1) All pr...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>115.</td>\n",
       "      <td>115. Short title and commencement\\n    (1) Thi...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    section                                               text  token_count\n",
       "0        2.  2. Purpose of Act\\nThe purpose of this Act is ...          200\n",
       "1        3.  3. Application and interpretation of Act\\n    ...          379\n",
       "2        4.  4. Lawful processing of personal information\\n...          655\n",
       "3        5.  5. Rights of data subjects\\nA data subject has...          472\n",
       "4        6.  6. Exclusions\\n    (1) This Act does not apply...          279\n",
       "..      ...                                                ...          ...\n",
       "135    111.  111. Fees\\n    (1) The Minister may, subject t...          111\n",
       "136    112.  112. Regulations\\n    (1) The Minister may, su...          423\n",
       "137    113.  113. Procedure for making regulations\\n    (1)...          380\n",
       "138    114.  114. Transitional arrangements\\n    (1) All pr...          207\n",
       "139    115.  115. Short title and commencement\\n    (1) Thi...           74\n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sectioned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectioned_df.to_csv(save_sectioned_df_to_file, encoding=\"utf-8\", sep=\"|\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     df_popia_definitions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39mdf_popia_definitions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefinition\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_ada_embedding)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#df_popia_definitions.to_csv(popia_definitions_and_embeddings_file, sep=\"|\", encoding='utf-8', index = False)\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mdf_popia_definitions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./inputs/popia_definitions.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Code\\chat\\popia\\env\\lib\\site-packages\\pandas\\core\\frame.py:2970\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2882\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2883\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2884\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2966\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2967\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2968\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2970\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2971\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2972\u001b[0m     path,\n\u001b[0;32m   2973\u001b[0m     engine,\n\u001b[0;32m   2974\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m   2975\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m   2976\u001b[0m     partition_cols\u001b[39m=\u001b[39mpartition_cols,\n\u001b[0;32m   2977\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   2978\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2979\u001b[0m )\n",
      "File \u001b[1;32me:\\Code\\chat\\popia\\env\\lib\\site-packages\\pandas\\io\\parquet.py:483\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m    481\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[1;32m--> 483\u001b[0m impl\u001b[39m.\u001b[39mwrite(\n\u001b[0;32m    484\u001b[0m     df,\n\u001b[0;32m    485\u001b[0m     path_or_buf,\n\u001b[0;32m    486\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m    487\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m    488\u001b[0m     partition_cols\u001b[39m=\u001b[39mpartition_cols,\n\u001b[0;32m    489\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m    490\u001b[0m     filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[0;32m    491\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    495\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32me:\\Code\\chat\\popia\\env\\lib\\site-packages\\pandas\\io\\parquet.py:197\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     merged_metadata \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexisting_metadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdf_metadata}\n\u001b[0;32m    195\u001b[0m     table \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39mreplace_schema_metadata(merged_metadata)\n\u001b[1;32m--> 197\u001b[0m path_or_handle, handles, filesystem \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    198\u001b[0m     path,\n\u001b[0;32m    199\u001b[0m     filesystem,\n\u001b[0;32m    200\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    201\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    202\u001b[0m     is_dir\u001b[39m=\u001b[39;49mpartition_cols \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    205\u001b[0m     \u001b[39misinstance\u001b[39m(path_or_handle, io\u001b[39m.\u001b[39mBufferedWriter)\n\u001b[0;32m    206\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(path_or_handle, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    207\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_handle\u001b[39m.\u001b[39mname, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m))\n\u001b[0;32m    208\u001b[0m ):\n\u001b[0;32m    209\u001b[0m     path_or_handle \u001b[39m=\u001b[39m path_or_handle\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32me:\\Code\\chat\\popia\\env\\lib\\site-packages\\pandas\\io\\parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    129\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[0;32m    132\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    140\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32me:\\Code\\chat\\popia\\env\\lib\\site-packages\\pandas\\io\\common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 739\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    742\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    743\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32me:\\Code\\chat\\popia\\env\\lib\\site-packages\\pandas\\io\\common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    602\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'inputs'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import src.embeddings\n",
    "importlib.reload(src.embeddings)\n",
    "from src.embeddings import get_ada_embedding, num_tokens_from_string\n",
    "\n",
    "def count_leading_spaces(s):\n",
    "    match = re.match(r'^\\s*', s)  # Matches leading whitespace\n",
    "    return len(match.group(0))\n",
    "\n",
    "add_embeddings = False\n",
    "definitions_to_process = '#Definition 1'\n",
    "if definitions_to_process == '#Definition 1':\n",
    "    popia_definitions_and_embeddings_file = \"./tmp/popia_definitions_with_embeddings.csv\"\n",
    "    exclude_first_line = True\n",
    "    start_line = 2\n",
    "else:\n",
    "    raise NotImplemented(\"Only implemented for popia Definition 1\")\n",
    "\n",
    "popia_manual_definitions = []\n",
    "#raw_list = non_text['Definition']['#Definition 1']\n",
    "raw_list = non_text['Definition'][definitions_to_process]\n",
    "number_of_spaces = count_leading_spaces(raw_list[start_line])\n",
    "if number_of_spaces % 4 != 0:\n",
    "    raise ValueError(f\"This line does not have an indent which is a multiple of 4: {raw_list[start_line]}\")\n",
    "\n",
    "current_line = raw_list[start_line]\n",
    "\n",
    "current_line_number_of_spaces = count_leading_spaces(current_line)\n",
    "if current_line_number_of_spaces != number_of_spaces:\n",
    "    print(f\"current_line_number_of_spaces: {current_line_number_of_spaces}\")\n",
    "    print(f'number_of_spaces: {number_of_spaces}')\n",
    "    raise ValueError(f\"This line does not have the correct indentation: {current_line}\")\n",
    "\n",
    "processing_table = False\n",
    "for line_number in range(start_line,len(raw_list)-1):\n",
    "    next_line = raw_list[line_number + 1]\n",
    "    next_line_number_of_spaces = count_leading_spaces(next_line)\n",
    "    if next_line_number_of_spaces % 4 != 0:\n",
    "        raise ValueError(f\"This line does not have an indent which is a multiple of 4: {next_line_number_of_spaces}\")\n",
    "\n",
    "    if current_line_number_of_spaces == next_line_number_of_spaces:\n",
    "        current_line = current_line.lstrip()\n",
    "        if \"|\" in current_line: # processing something with table formatting\n",
    "            processing_table = True\n",
    "            split_line = [x.strip() for x in current_line.split(\"|\")]\n",
    "            popia_manual_definitions.append(split_line)\n",
    "        else:\n",
    "            popia_manual_definitions.append(current_line)\n",
    "        current_line = next_line\n",
    "    else:\n",
    "        current_line = current_line + \"\\n\" + next_line\n",
    "\n",
    "# add the last entry\n",
    "current_line = current_line.lstrip()\n",
    "if processing_table:\n",
    "    split_line = [x.strip() for x in current_line.split(\"|\")]\n",
    "    popia_manual_definitions.append(split_line)\n",
    "else:\n",
    "    popia_manual_definitions.append(current_line.lstrip())\n",
    "\n",
    "if processing_table:\n",
    "    headings = popia_manual_definitions[0]\n",
    "    popia_manual_definitions.pop(0)\n",
    "    df_reg23_definitions = pd.DataFrame(popia_manual_definitions, columns=headings) \n",
    "else:\n",
    "    df_reg23_definitions = pd.DataFrame(popia_manual_definitions, columns=[\"Definition\"])\n",
    "\n",
    "\n",
    "if add_embeddings:\n",
    "    df_reg23_definitions[\"Embedding\"] =df_reg23_definitions[\"Definition\"].apply(get_ada_embedding)\n",
    "\n",
    "df_reg23_definitions[\"source\"] = 'all'\n",
    "\n",
    "#df_reg23_definitions.to_csv(popia_definitions_and_embeddings_file, encoding = \"utf-8\", sep = \"|\", index = False)\n",
    "df_reg23_definitions.to_parquet(\"./inputs/popia_definitions.parquet\", engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg23_definitions = pd.read_parquet(\"./inputs/popia_definitions.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg23_definitions['source'] = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg23_definitions.to_parquet(\"./inputs/popia_definitions.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section headings are also a good index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 is to remove all text that is not tagged as a heading leaving only the index and headings\n",
    "toc_file = \"./tmp/section_numbers_and_headings.txt\" # Note this is a temporary file and will be deleted in a few cells time\n",
    "df = df_reg23\n",
    "index = index_reg23\n",
    "\n",
    "\n",
    "written_references = set() # only write each reference once\n",
    "with open(toc_file, 'w', encoding = 'utf-8') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        if row['full_reference'] not in written_references:\n",
    "            written_references.add(row['full_reference'])\n",
    "            s = ' ' * row['Indent'] * 4 + row['Reference']\n",
    "            # skip the text for into and legal\n",
    "            if row['Heading'] and (row['Indent'] == 0 and row['Text'].strip() in index.exclusion_list):\n",
    "                s = s\n",
    "            elif row['Heading']:               \n",
    "                s += \" \" + row['Text']\n",
    "            f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2) Remove all lines that do not have text. Since the only text is for the headers, this removes everything that is not a header.\n",
    "#         Note however that the remaining \"headers\" will not contain the (#headers) markdown so will be treated as text\n",
    "import src.file_tools\n",
    "importlib.reload(src.file_tools)\n",
    "from src.file_tools import process_regulations\n",
    "\n",
    "excon_headers = './tmp/non_empty_headings_excon.txt'  # Note this is a temporary file and will be deleted in a few cells time\n",
    "files_as_list = []\n",
    "files_as_list.append(toc_file)\n",
    "df_toc, non_text_toc = process_regulations(files_as_list, valid_index_checker=index_reg23, non_text_labels=non_text_labels)\n",
    "\n",
    "for index, row in df_toc.iterrows():\n",
    "    if row['Text'] != \"\":\n",
    "        df_toc.at[index, 'Heading'] = True\n",
    "\n",
    "tree_toc = build_tree_for_regulation(\"popia_toc\", df_toc, valid_index_checker=index_reg23)\n",
    "l = tree_toc._list_node_children(tree_toc.root)\n",
    "with open(excon_headers, 'w', encoding = 'utf-8') as f:\n",
    "    f.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3) The file that contains the headers (now as text because they are missing the (#heading) markdown) and load it up\n",
    "#         as if it were the regs themselves\n",
    "files_as_list = []\n",
    "files_as_list.append(excon_headers)\n",
    "df_non_empty_toc, non_text_toc = process_regulations(files_as_list, valid_index_checker=index_reg23, non_text_labels=non_text_labels)\n",
    "for index, row in df_non_empty_toc.iterrows():\n",
    "    if row['Text'] != \"\":\n",
    "        df_non_empty_toc.at[index, 'Heading'] = True\n",
    "\n",
    "non_empty_tree_toc = build_tree_for_regulation(\"Excon\", df_non_empty_toc, valid_index_checker=index_reg23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popia Act contains 114 section headings\n"
     ]
    }
   ],
   "source": [
    "# Construct the full reference of each heading and use these as a key for a dictionary where the heading is the value\n",
    "import os\n",
    "\n",
    "def get_leaf_headings(root):\n",
    "    leaf_headings = {}\n",
    "\n",
    "    def recurse(node, heading):\n",
    "        # Add \". \" only if it's not the root node and the node's heading_text is not empty\n",
    "        new_heading = heading + (\". \" + node.heading_text if heading and node.heading_text else node.heading_text)\n",
    "        if not node.children:  # This is a leaf node.\n",
    "            leaf_headings[node.full_node_name] = new_heading\n",
    "        else:  # This is not a leaf node. We continue the recursion.\n",
    "            for child in node.children:\n",
    "                recurse(child, new_heading)\n",
    "\n",
    "    recurse(root, '')  # We start the recursion from the root, with an empty heading.\n",
    "    return leaf_headings\n",
    "\n",
    "leaf_headings = get_leaf_headings(non_empty_tree_toc.root)\n",
    "df_section_headings = pd.DataFrame(list(leaf_headings.items()), columns=['section', 'text'])\n",
    "print(f'Popia Act contains {len(leaf_headings)} section headings')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_section_headings.to_csv(headings_index_file, encoding = \"utf-8\", sep = \"|\", index = False)\n",
    "if os.path.exists(toc_file):\n",
    "    os.remove(toc_file)\n",
    "if os.path.exists(excon_headers):\n",
    "    os.remove(excon_headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.</td>\n",
       "      <td>Purpose of Act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.</td>\n",
       "      <td>Application and interpretation of Act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.</td>\n",
       "      <td>Lawful processing of personal information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.</td>\n",
       "      <td>Rights of data subjects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.</td>\n",
       "      <td>Exclusions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>111.</td>\n",
       "      <td>Fees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>112.</td>\n",
       "      <td>Regulations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>113.</td>\n",
       "      <td>Procedure for making regulations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>114.</td>\n",
       "      <td>Transitional arrangements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>115.</td>\n",
       "      <td>Short title and commencement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    section                                       text\n",
       "0        2.                             Purpose of Act\n",
       "1        3.      Application and interpretation of Act\n",
       "2        4.  Lawful processing of personal information\n",
       "3        5.                    Rights of data subjects\n",
       "4        6.                                 Exclusions\n",
       "..      ...                                        ...\n",
       "109    111.                                       Fees\n",
       "110    112.                                Regulations\n",
       "111    113.           Procedure for making regulations\n",
       "112    114.                  Transitional arrangements\n",
       "113    115.               Short title and commencement\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index.to_parquet(\"./inputs/popia_index.parquet\", engine='pyarrow')\n",
    "# NOTE: At some point the heading sections were given an additional \"0\" at the end of the section which causes issues.\n",
    "#        Confirm all the headings are \"2.\" and not \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "\n",
    "df_index = pd.read_parquet(section_questions_with_embeddings, engine='pyarrow')\n",
    "df_index = df_index[df_index[\"text\"] != \"\"] # remove rows that have 'text' == \"\"\n",
    "# the 'text' column for the questions may contain multiple questions separated by a \"|\". The next line expands these rows\n",
    "# so the value in 'text' only contains one question\n",
    "df_index = df_index.drop(\"text\", axis=1).join(df_index[\"text\"].str.split(\"|\", expand=True).stack().reset_index(level=1, drop=True).rename(\"text\"))\n",
    "df_index.reset_index(drop=True, inplace=True)\n",
    "df_index[\"source\"] = \"question\"\n",
    "\n",
    "df_tmp = pd.read_parquet(section_summary_with_embeddings, engine='pyarrow')\n",
    "df_tmp[\"source\"] = \"summary\"\n",
    "\n",
    "df_tmp_2 = pd.read_csv(headings_index_file, encoding = \"utf-8\", sep = \"|\")\n",
    "df_tmp_2[\"source\"] = \"heading\"\n",
    "\n",
    "df_index = pd.concat([df_index, df_tmp, df_tmp_2], ignore_index = True)\n",
    "df_index = df_index[df_index[\"text\"]!= \"\"]\n",
    "df_index = df_index[df_index[\"text\"].notna()] # Remove any NaN's\n",
    "df_index.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#df_index['Embedding'] = df_index['text'].apply(get_ada_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index['section'] = df_index['section'].astype(str)\n",
    "df_index['text'] = df_index['text'].astype(str)\n",
    "df_index['source'] = df_index['source'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index['Embedding'] = df_index['text'].apply(get_ada_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "section      object\n",
       "text         object\n",
       "source       object\n",
       "Embedding    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index.to_parquet(\"./inputs/popia_index.parquet\", engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.read_parquet(\"./inputs/popia_index.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.</td>\n",
       "      <td>What is the purpose of the Protection of Perso...</td>\n",
       "      <td>question</td>\n",
       "      <td>[-0.00897529348731041, -0.004865928087383509, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>What rights does the Act provide individuals ...</td>\n",
       "      <td>question</td>\n",
       "      <td>[-0.0016322160372510552, 0.0023074529599398375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.</td>\n",
       "      <td>What kind of regulatory body is established b...</td>\n",
       "      <td>question</td>\n",
       "      <td>[0.004339318256825209, -0.012121826410293579, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.</td>\n",
       "      <td>What data or information does POPIA apply to?</td>\n",
       "      <td>question</td>\n",
       "      <td>[0.0023713144473731518, 0.0032313510309904814,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.</td>\n",
       "      <td>How does the POPIA interact with other laws r...</td>\n",
       "      <td>question</td>\n",
       "      <td>[0.003651085076853633, 0.010420947335660458, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>111.0</td>\n",
       "      <td>Fees</td>\n",
       "      <td>heading</td>\n",
       "      <td>[-0.0012755942298099399, 0.006350830662995577,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>112.0</td>\n",
       "      <td>Regulations</td>\n",
       "      <td>heading</td>\n",
       "      <td>[0.0018349532037973404, -0.012941248714923859,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>113.0</td>\n",
       "      <td>Procedure for making regulations</td>\n",
       "      <td>heading</td>\n",
       "      <td>[0.009133221581578255, 0.004106280393898487, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>114.0</td>\n",
       "      <td>Transitional arrangements</td>\n",
       "      <td>heading</td>\n",
       "      <td>[0.00048234881251119077, -0.011674447916448116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>115.0</td>\n",
       "      <td>Short title and commencement</td>\n",
       "      <td>heading</td>\n",
       "      <td>[-0.004542532376945019, -0.0018438507104292512...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    section                                               text    source  \\\n",
       "0        2.  What is the purpose of the Protection of Perso...  question   \n",
       "1        2.   What rights does the Act provide individuals ...  question   \n",
       "2        2.   What kind of regulatory body is established b...  question   \n",
       "3        3.     What data or information does POPIA apply to?   question   \n",
       "4        3.   How does the POPIA interact with other laws r...  question   \n",
       "..      ...                                                ...       ...   \n",
       "481   111.0                                               Fees   heading   \n",
       "482   112.0                                        Regulations   heading   \n",
       "483   113.0                   Procedure for making regulations   heading   \n",
       "484   114.0                          Transitional arrangements   heading   \n",
       "485   115.0                       Short title and commencement   heading   \n",
       "\n",
       "                                             Embedding  \n",
       "0    [-0.00897529348731041, -0.004865928087383509, ...  \n",
       "1    [-0.0016322160372510552, 0.0023074529599398375...  \n",
       "2    [0.004339318256825209, -0.012121826410293579, ...  \n",
       "3    [0.0023713144473731518, 0.0032313510309904814,...  \n",
       "4    [0.003651085076853633, 0.010420947335660458, 0...  \n",
       "..                                                 ...  \n",
       "481  [-0.0012755942298099399, 0.006350830662995577,...  \n",
       "482  [0.0018349532037973404, -0.012941248714923859,...  \n",
       "483  [0.009133221581578255, 0.004106280393898487, -...  \n",
       "484  [0.00048234881251119077, -0.011674447916448116...  \n",
       "485  [-0.004542532376945019, -0.0018438507104292512...  \n",
       "\n",
       "[486 rows x 4 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 250 lines\n",
      "Completed 260 lines\n",
      "Completed 270 lines\n",
      "Completed 280 lines\n",
      "Completed 290 lines\n",
      "Completed 300 lines\n",
      "Completed 310 lines\n",
      "Completed 320 lines\n",
      "Completed 330 lines\n"
     ]
    }
   ],
   "source": [
    "# if there is an error somewhere in the generation of the embedding and you need to find it, this is a hacky way to do that\n",
    "increment = 10\n",
    "for i in range(0, len(df_index), increment):\n",
    "    chunk = df_index.iloc[i:i+increment].copy()\n",
    "    chunk[\"Embedding\"] = chunk[\"text\"].apply(get_ada_embedding)\n",
    "    df_index.loc[chunk.index, \"Embedding\"] = chunk[\"Embedding\"]\n",
    "    print(f\"Completed {i+increment} lines\")\n",
    "\n",
    "\n",
    "#df_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e8c950878b7c68fcd31728edc667d73f7a883c04481614db908b7ff2bdf4c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
